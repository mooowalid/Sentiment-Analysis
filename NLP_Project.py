# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CWGuJZID9_YTfdHFyOeyfdB-mPOwR9eQ
"""

import pandas as pd
import nltk
nltk.download('punkt')  # Download the tokenizer data
nltk.download('punkt_tab')
from nltk.tokenize import word_tokenize
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
nltk.download('wordnet')
nltk.download('omw-1.4')
from nltk.stem import WordNetLemmatizer
from nltk import pos_tag

nltk.download('averaged_perceptron_tagger')
nltk.download('averaged_perceptron_tagger_eng')
import spacy
import tensorflow as tf
import tensorflow
from tensorflow import keras
from keras.layers import Dense
from sklearn.feature_extraction.text import CountVectorizer
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np
from keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
import re
import string
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout

data = pd.read_csv('/content/LabeledText.csv')

data

data.duplicated().sum()

data.dropna()

data = data.applymap(lambda x: x.lower() if isinstance(x, str) else x)

def remove_punctuation(text):
    return text.translate(str.maketrans('', '', string.punctuation))
data = data.applymap(lambda x: remove_punctuation(x) if isinstance(x, str) else x)

def remove_special_chars(text):
    return re.sub(r'[^A-Za-z0-9\s]', '', text)

data = data.applymap(lambda x: remove_special_chars(x) if isinstance(x, str) else x)

newData=data[['Caption', 'LABEL']]

newData

newData['Caption'] = newData['Caption'].str.replace(r'\d+', '', regex=True)

newData['tokens'] = newData['Caption'].apply(lambda x: word_tokenize(x))  # Tokenize each row of the text column

newData

stop_words = set(stopwords.words('english'))
newData['STOPREMOVE'] = newData['Caption'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))

newData

ps = PorterStemmer()
def stem_tokens(tokens):
    return [ps.stem(word) for word in tokens]
newData['stemmed_text'] = newData['tokens'].apply(stem_tokens)

newData

# Initialize WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

# Example column containing text data in your dataset
# Replace 'text_column' with the actual column name in newData
Caption = 'Caption'

# Function to lemmatize text
def lemmatize_text(text):
    # Tokenize the text
    tokens = nltk.word_tokenize(text)
    # Perform lemmatization with POS tagging for better accuracy
    lemmatized_words = [
        lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens
        if word.isalpha()  # Ignore punctuation
    ]
    return ' '.join(lemmatized_words)

# Function to map NLTK POS tags to WordNet POS tags
def get_wordnet_pos(word):
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {'J': 'a', 'N': 'n', 'V': 'v', 'R': 'r'}
    return tag_dict.get(tag, 'n')  # Default to noun

# Apply lemmatization to the dataset
newData['lemmatized_text'] = newData[Caption].apply(lemmatize_text)

# Display a sample of the updated dataset
print(newData[['Caption', 'lemmatized_text']].head())

newData

# Initialize CountVectorizer for Bag of Words (BoW)
vectorizer = CountVectorizer()

# Fit the vectorizer and transform the text into BoW features
X = vectorizer.fit_transform(newData['Caption'])

# Convert the BoW feature matrix into a DataFrame for better readability
bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())
bow_df['Caption'] = newData['Caption']
bow_df.set_index('Caption', inplace=True)

bow_df

bow_df['Caption'] = newData['Caption']

max_len = max(newData['tokens'].apply(len))
def pad_sequence(seq, max_len, pad_token="0"):
    return seq + [pad_token] * (max_len - len(seq))

newData['padded'] = newData['tokens'].apply(lambda x: pad_sequence(x, max_len))
print(newData)

newData

# Map sentiment labels to integers: Negative=0, Neutral=1, Positive=2
label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}
newData['LABEL'] = newData['LABEL'].map(label_mapping)

import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, GRU, Dense
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split

# Example: Assume 'data' is your input text and 'labels' is the corresponding sentiment labels
# Replace this with your actual dataset loading
data = newData['Caption']
labels = newData['LABEL']

# Step 1: Convert labels to one-hot encoding
labels = np.array(labels)
y = to_categorical(labels, num_classes=3)

# Step 2: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.1, random_state=42)

# Step 3: Tokenize and pad the sequences
tokenizer = Tokenizer(num_words=10000)
tokenizer.fit_on_texts(X_train)

X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)

# Step 4: Padding sequences to ensure uniform length
max_sequence_length = 15  # You can adjust this based on your dataset
X_train_pad = pad_sequences(X_train_seq, maxlen=max_sequence_length)
X_test_pad = pad_sequences(X_test_seq, maxlen=max_sequence_length)

# Step 5: Build the GRU model
model = Sequential()
model.add(Embedding(input_dim=10000, output_dim=128, input_length=max_sequence_length))
model.add(GRU(128, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(3, activation='softmax'))  # 3 classes: Negative, Neutral, Positive

# Step 6: Compile the model
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

# Step 7: Train the model
model.fit(X_train_pad, y_train, epochs=5, batch_size=32, validation_data=(X_test_pad, y_test))

# Step 8: Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test_pad, y_test)
print(f"Test Accuracy: {test_accuracy}")

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Assuming you have already trained the GRU model and obtained predictions
# y_pred is the predicted probabilities from your GRU model (model.predict)
# y_test is the true labels (in one-hot encoding)

# Step 1: Convert predicted probabilities to class labels
# If the model outputs probabilities, convert them to the class with the highest probability
y_pred = model.predict(X_test_pad)

y_pred_class = np.argmax(y_pred, axis=1)

# Step 2: Convert the one-hot encoded true labels to class labels
y_true_class = np.argmax(y_test, axis=1)

# Step 3: Calculate Accuracy
accuracy = accuracy_score(y_true_class, y_pred_class)
print(f"Accuracy: {accuracy}")

# Step 4: Calculate Precision, Recall, and F1-Score (weighted average)
precision = precision_score(y_true_class, y_pred_class, average='weighted')
recall = recall_score(y_true_class, y_pred_class, average='weighted')
f1 = f1_score(y_true_class, y_pred_class, average='weighted')

print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1-Score: {f1}")

# Step 5: Print Classification Report (includes precision, recall, and f1-score for each class)
print("Classification Report:")
print(classification_report(y_true_class, y_pred_class, target_names=['Negative', 'Neutral', 'Positive']))

# Step 6: Confusion Matrix (to visualize the performance per class)
cm = confusion_matrix(y_true_class, y_pred_class)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

import numpy as np
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Example new data (replace this with your actual unseen text data)
new_data = ["This is a great product!", "I did not like the service.", "The movie was okay, not bad."]

# Step 1: Tokenize and pad the new data
new_data_sequences = tokenizer.texts_to_sequences(new_data)  # Tokenize the new data using the same tokenizer
new_data_padded = pad_sequences(new_data_sequences, maxlen=max_sequence_length)  # Pad sequences

# Step 2: Get predictions for the new data
y_pred_new = model.predict(new_data_padded)  # Get predicted probabilities

# Step 3: Convert predicted probabilities to class labels
y_pred_new_class = np.argmax(y_pred_new, axis=1)

# Step 4: Map predicted class indices to label names
class_labels = ['Neutral', 'Negative', 'Positive']
predicted_labels = [class_labels[i] for i in y_pred_new_class]

# Step 5: Print the input text with the predicted labels
for text, label in zip(new_data, predicted_labels):
    print(f'"{text}" -> {label}')